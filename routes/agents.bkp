const express = require('express');
const router = express.Router();
const mongoose = require('mongoose');
const ObjectId = mongoose.Types.ObjectId;
const axios = require('axios');

//For the PROVIDERfileHelper
const path = require('path');
const jsonfile = require('jsonfile')
const mkdirp = require('mkdirp');

//const agentAdapter = require('../adapters/agent');

//For the CONSUMERexport
const { parse } = require('json2csv');
const fs = require('fs');
const archiver = require('archiver');
const FormData = require('form-data');


//FOR AGENTgetAgent
const { Agent } = require('../models/agents');

//For the AGENTcreateJob function
const { Job } = require('../models/jobs');

//Agent ID = 5d5b050100f2714d4b2dfa8d


//TODO - update this to a POST - currently using get so i can quickly invoke from the browser
    //will need to change req.query.params to body params
router.get('/jobs', function (req, res, next) {
    // 1. Call the agent.createJob function
    // 2. Pass the result to agent.getAgent
    // 3. Pass the results of createJob and getAgent to <provider>.import (in this case <provider>is eduapi.import)
    //     3.a Store all this data in an object then store in a .json file - pass this file name as a param
    // 4. Call the <consumer>.export (in this case <consumer> is canvas.export)
    // 5. Upon completion call a close out to save the success/failure status to the job.status field in the DB
    
    data = {}

    AGENTcreateJob(req.query.agentId)
        .then((job) => {
            res.json({
                status: 'ok',
                job
            })

            //Pass data downstream
            data.job = job
            return data

        })
        .then((data) => AGENTgetAgent(data))
        .then((data) => PROVIDERimport(data))
        .then((data) => PROVIDERfileHelper(data))
        .then((data) => CONSUMERexport(data))
        .then((data) => CONSUMERapiRequest(data))
        .then((data) => AGENTclose(data))
        // .then((doc) => {
        //     //next thing to do
        //     // probably start the provider import
        //     //then export to canvas

        //     console.log('hit next then', doc)
        // })
        .catch((e) => {
            return next ({
                message: e.message || 'There was an issue creating the job',
                status: e.status || 500,
                stack: e.stack || ''
            })
        })
})

function AGENTcreateJob (agentId) {
    var job = new Job({ agentId });

    return job.save()
        .then((job) => {
            return Promise.resolve(job)
        })
        .catch((e) => {
            return Promise.reject('Failed to create Job')
        });
}

function AGENTgetAgent (data) {
    return Agent.get(data.job.agentId)
        .then((agent) => {
            data.agent = agent
            return data
        })
        .catch((e) => {
            return Promise.reject('Failed to fetch agent')
        });
}

function PROVIDERimport (data) {
    data.data = {}

    var promises = ['persons', 'educations', 'educationOfferings', 'offeringAssociations'].map(function(dType){
        data.data[dType] = []

        return PROVIDERfetchHelper(data, data.agent.providerApiUrl, dType, 2, 0)
            .then((res) => {
                return data.data[dType]
            })
      })
      return Promise.all(promises).then(function(results) {
          return data
      })
}

function PROVIDERfetchHelper (data, host_path, endpoint, limit, offset) {
    return axios.get(`${host_path}/${endpoint}?limit=${limit}&offset=${offset}`)
        .then((res) => {
            //Concat regardless of next step
            data.data[endpoint] = data.data[endpoint].concat(res.data[endpoint])

            //Loop back through this func for more data
            if (res.data[endpoint].length === limit) {
                return PROVIDERfetchHelper(data, host_path, endpoint, limit, (limit + offset))
            }

            //res.data is either 0 or lower than the limit, already ran concat on this so now just return data
            return data
        })
}

//NOTE - this operation is synchronous - was lazy and didn't want to deal with more async hell
function PROVIDERfileHelper(data) {
    //TODO - decide if I'm going to move this directory outside the project - nodemon sees it as a change and restarts the server, not a big deal but logs don't display
    let dir = path.join(__dirname, `../data/${data.agent._id}`)
    let filePath = `${dir}/original_${data.job._id}.json`

    mkdirp.sync(dir, (err) => {
        if (err) return Promise.reject('Error creating directory for data')
        //Success
        console.log('Directory created');
    })

    jsonfile.writeFileSync(filePath, data.data, function (err) {
        if (err) return Promise.reject('Error writing the .json file')
        //Success
        console.log(`JSON file written and saved in /data/${data.agent._id} dir`);
    })

    //Nothing new written just returning data to pass through to next step in loop
    return data
}

//EDUAPI spec specific
    //not storing data in DB or writing to a specific middlware format, consumer exports will run through a specific provider format transform then run through a standard exporter 
function CONSUMERexport (data) {
    let dir = path.join(__dirname, `../data/${data.agent._id}/transformed`)

    mkdirp.sync(dir, (err) => {
        if (err) return Promise.reject('Error creating transformed directory')
        //Success
        console.log('Transformed directory created');
    })


    //order - users, accounts, terms, courses, sections, enrollments

    //Considering mapping through an array of different files but lazy so doing this...
    return usersCsv(data.data.persons, dir)
        .then(() => {
            return coursesCsv(data.data.educationOfferings, dir)
                .then(() => {
                    return sectionsCsv(data.data.educationOfferings, dir)
                        .then(() => {
                            return enrollmentsCsv(data.data.offeringAssociations, dir)
                                .then(() => {
                                    return zipper(data, dir)
                                        .then(() => {
                                            //TODO - just setting a marker because this is almost certainly going to break because of async issues
                                            return data
                                        })
                                })
                        })
                })
        })
}

function zipper(data, dir) {
    return new Promise(function(resolve, reject) {
        let zipPath = path.join(__dirname, `../data/${data.agent._id}`)

        // create a file to stream archive data to.
        var output = fs.createWriteStream(`${zipPath}/data.zip`);
        var archive = archiver('zip', {
            zlib: { level: 9 } // Sets the compression level.
        });
        
        // listen for all archive data to be written
        // 'close' event is fired only when a file descriptor is involved
        output.on('close', function() {
            resolve('Archiver has finalized and closed')
            // console.log(archive.pointer() + ' total bytes');
            // console.log('archiver has been finalized and the output file descriptor has closed.');
        });
        
        // This event is fired when the data source is drained no matter what was the data source.
        // It is not part of this library but rather from the NodeJS Stream API.
        // @see: https://nodejs.org/api/stream.html#stream_event_end
        output.on('end', function() {
            console.log('Data has been drained');
        });
        
        // good practice to catch warnings (ie stat failures and other non-blocking errors)
        archive.on('warning', function(err) {
            reject(err);
            // if (err.code === 'ENOENT') {
            //     // log warning
            //     console.log(err);
            // } else {
            //     // throw error
            //     throw err;
            // }
        });
        
        // good practice to catch this error explicitly
        archive.on('error', function(err) {
            // throw err;
            reject(err);
        });

        archive.pipe(output);
        archive.directory(dir, false);
        archive.finalize();
    })
}

function usersCsv(users, dir) {
    var usersPromises = users.map(function(d){
        return Promise.resolve({
            user_id: d._id, //TODO - whenever I change the user obj to read sourcedId instead of _id i'll need to update this and probably a few other places
            login_id: d.username,
            first_name: d.name[0].givenName,
            last_name: d.name[0].surname,
            status: d.status
        })
    })
    return Promise.all(usersPromises).then(function(results) {
        const fields = ['user_id', 'login_id', 'first_name', 'last_name', 'status'];
        const opts = { fields };

        try {
            const csv = parse(results, opts)

            fs.writeFileSync(`${dir}/users.csv`, csv, function(err) {
                if (err) return Promise.reject('Failed to write users.csv file')
            
                console.log('Users.csv was saved to /transformed dir');
            })

            return Promise.resolve()
        } catch (e) {
            return Promise.reject('Failed to users.csv')
        }
    })
}

function coursesCsv(users, dir) {
    var usersPromises = users.map(function(d){
        let status
        
        switch (d.registrationStatus) {
            case 'open':
                status = 'active';
                break;
            default:
                status = 'completed';
        }

        return Promise.resolve({
            course_id: d.sourcedId,
            short_name: d.title,
            long_name: d.title,
            // term_id: d.academicSession.sourcedId, //commenting this because we haven't finished the terms endpoint so I haven't pulled this data in - it'll fail on canvas import
                //don't forget to add it to the fields array below as well
            account_id: d.education.sourcedId,
            status
        })
    })
    return Promise.all(usersPromises).then(function(results) {
        const fields = ['course_id', 'short_name', 'long_name', 'account_id', 'status'];
        const opts = { fields };

        try {
            const csv = parse(results, opts)

            fs.writeFileSync(`${dir}/courses.csv`, csv, function(err) {
                if (err) return Promise.reject('Failed to write courses.csv file')
            
                console.log('Courses.csv was saved to /transformed dir');
            })

            return Promise.resolve()
        } catch (e) {
            return Promise.reject('Failed to create courses.csv')
        }
    })
}

function sectionsCsv(users, dir) {
    var usersPromises = users.map(function(d){
        let status
        
        switch (d.registrationStatus) {
            case 'open':
                status = 'active';
                break;
            default:
                status = 'completed';
        }

        return Promise.resolve({
            section_id: d.sourcedId,
            course_id: d.sourcedId,
            name: d.title,
            status
        })
    })
    return Promise.all(usersPromises).then(function(results) {
        const fields = ['section_id', 'course_id', 'name', 'status'];
        const opts = { fields };

        try {
            const csv = parse(results, opts)

            fs.writeFileSync(`${dir}/sections.csv`, csv, function(err) {
                if (err) return Promise.reject('Failed to write sections.csv file')
            
                console.log('Sections.csv was saved to /transformed dir');
            })

            return Promise.resolve()
        } catch (e) {
            return Promise.reject('Failed to create sections.csv')
        }
    })
}

function enrollmentsCsv(users, dir) {
    var usersPromises = users.map(function(d){

        //Will probably need this soon so just commenting it out - swap the status in the return object below
        // let status
        
        // switch (d.registrationStatus) {
        //     case 'active':
        //         status = 'active';
        //         break;
        //     default:
        //         status = 'inactive';
        // }

        return Promise.resolve({
            section_id: d.educationOfferingId.sourcedId,
            user_id: d.personId.sourcedId,
            role: d.role,
            status: d.status
        })
    })
    return Promise.all(usersPromises).then(function(results) {
        const fields = ['section_id', 'user_id', 'role', 'status'];
        const opts = { fields };

        try {
            const csv = parse(results, opts)

            fs.writeFileSync(`${dir}/enrollments.csv`, csv, function(err) {
                if (err) return Promise.reject('Failed to write enrollments.csv file')
            
                console.log('Enrollments.csv was saved to /transformed dir');
            })

            return Promise.resolve()
        } catch (e) {
            return Promise.reject('Failed to create enrollments.csv')
        }
    })
}

function CONSUMERapiRequest(data) {
    let formData = fs.createReadStream(path.join(__dirname,  `../data/${data.agent._id}/data.zip`));
    const form = new FormData();
    form.append('attachment', formData, 'data.zip');
    axios.post(`${data.agent.consumerApiUrl}/accounts/1/sis_imports.json?import_type=instructure_csv`, form, {
    headers: {
        ...form.getHeaders(),
        Authorization: `Bearer ${data.agent.consumerToken}`
    },
    }).then(result => {
        // Handle result…
        console.log(result.data);

        return data
    });
    
    //Dummy Data Importer
    // let formData = fs.createReadStream(path.join(__dirname, `../data/dummy2.zip`));
    // const form = new FormData();
    // form.append('attachment', formData, 'dummy.zip');
    // axios.post(`${data.agent.consumerApiUrl}/accounts/1/sis_imports.json?import_type=instructure_csv`, form, {
    // headers: {
    //     ...form.getHeaders(),
    //     Authorization: `Bearer ${data.agent.consumerToken}`
    // },
    // }).then(result => {
    //     // Handle result…
    //     console.log(result.data);
    // });

    // return data
}

function AGENTclose(data) {
    //Consider removing the zip and transformed data
        //Probably a good idea since we cannot guarantee a full dataset return from the SIS api
    Job.get(data.job._id)
        .then((job) => {
            job.status = 'complete'

            job.save()
                .then((job) => {
                    //   return res.json({
                    //     job,
                    //     status: 'ok'
                    //   })
                    console.log(`Success, Job ${data.job._id} complete`);
                })
                .catch((e) => {
                    return next({
                        status: 500,
                        message: 'Failed to update job. Internal server error.',
                        stack: e
                    })
                })
        })
        .catch((e) => {
            return next({
                message: e.message,
                status: e.status,
                stack: e
            })
        })
}

module.exports = router;
